# ğŸ¥ Medibot â€” Retrieval-Augmented Medical Question Answering System

Medibot is a retrieval-augmented medical question-answering system that combines semantic search with controlled language model generation to produce answers grounded in real medical sources.

Unlike generic chatbots, Medibot **retrieves first and generates second** â€” every response is traceable to a specific page in the source encyclopedia, not fabricated from memorized patterns.

---

## ğŸ“¸ Screenshots

<div align="center">

### Welcome Screen
<img src="assets/images/welcome_screen.jpeg" width="900" />

### Medical Q&A with Source Citations
<img src="assets/images/chat_response.jpeg" width="900" />

### Source Attribution
<img src="assets/images/source_citations.jpeg" width="900" />

</div>

---

## ğŸ¯ Why Medibot Exists

Large language models are powerful â€” but dangerous in healthcare when they hallucinate. Medibot was built to explore how **Retrieval-Augmented Generation (RAG)** can solve this:

- **Reduce hallucinations** by forcing the model to read before it answers
- **Ground every response** in verifiable, cited medical text
- **Say "I don't know"** when information is genuinely missing
- **Build trust** through source transparency

This is an educational and research project. It demonstrates the same architectural principles used in production medical AI systems â€” at a scale that's clear, learnable, and reproducible.

---

## ğŸ§  What Medibot Does

| Capability | How |
|---|---|
| Answers medical questions | Via a conversational web interface |
| Retrieves relevant passages | Semantic search over 3,400 indexed chunks |
| Grounds responses | LLM prompt is constrained to retrieved context only |
| Cites sources | Every answer includes document name and page numbers |
| Declines gracefully | Returns a safe fallback when data is missing |
| Runs fully local | No API keys â€” models execute on your machine |
| Auto-detects hardware | Uses Apple Silicon MPS, NVIDIA CUDA, or CPU automatically |

---

## ğŸ“š Knowledge Base

| Property | Value |
|---|---|
| **Source** | Gale Encyclopedia of Medicine, 2nd Edition |
| **Volume** | Volume 1 |
| **Topic Coverage** | Aâ€“B only |
| **Pages** | ~700 |
| **Indexed Chunks** | ~3,400 |
| **Vector Index** | FAISS (dense vector similarity search) |
| **Embedding Dimension** | 384 |

> âš ï¸ **Scope Limitation:** The corpus covers **Aâ€“B topics only**. When a question falls outside this range, Medibot responds:
> *"I am not certain based on the provided medical sources."*
>
> This behavior is intentional â€” not a bug.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER QUESTION                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CHAINLIT WEB UI                         â”‚
â”‚              (conversational interface)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FAISS VECTOR SEARCH                          â”‚
â”‚        semantic similarity over 3,400 chunks              â”‚
â”‚              retrieves top-k = 3                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             CONTEXT ASSEMBLY                              â”‚
â”‚     retrieved chunks â†’ structured prompt                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FLAN-T5 LLM GENERATION                         â”‚
â”‚     generates answer using ONLY retrieved context         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FINAL RESPONSE                               â”‚
â”‚         answer + source citations + disclaimer            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Principles

| Principle | How It's Enforced |
|---|---|
| **Retrieval first** | Vector search executes before any generation step |
| **No hallucination** | Prompt explicitly restricts the model to retrieved context |
| **Source transparency** | Page numbers and document names shown with every answer |
| **Corpus awareness** | System detects and communicates when data is missing |

---

## ğŸ› ï¸ Tech Stack

### Models

| Model | Role | Size |
|---|---|---|
| `sentence-transformers/all-MiniLM-L6-v2` | Converts text into 384-dimensional vectors | ~80 MB |
| `google/flan-t5-base` | Generates answers from retrieved context | ~900 MB |

### Libraries

| Library | Role |
|---|---|
| **LangChain** | Orchestrates the full RAG pipeline (retriever â†’ prompt â†’ LLM) |
| **FAISS** | Stores and searches 3,400 text embeddings at runtime |
| **Sentence-Transformers** | Runs the embedding model |
| **Hugging Face Transformers** | Runs Flan-T5 locally |
| **Chainlit** | Provides the interactive chat UI |
| **PyPDF** | Extracts text from the encyclopedia PDF |
| **PyTorch** | Model inference with hardware acceleration |

### Hardware

| Device | Support | How |
|---|---|---|
| **Apple Silicon (M1/M2/M3)** | âœ… Native | PyTorch MPS â€” auto-detected at startup |
| **NVIDIA GPU** | âœ… Supported | PyTorch CUDA â€” auto-detected at startup |
| **CPU** | âœ… Fallback | Works on any machine, slower inference |

> Everything runs locally. No external APIs or paid services are required. Device selection is automatic â€” `config.py` detects MPS, CUDA, or CPU at import time.

---

## ğŸ“‚ Project Structure

```
Medibot/
â”‚
â”œâ”€â”€ main.py                          # Chainlit app â€” entry point
â”œâ”€â”€ chainlit.md                      # Welcome screen content
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env.example                     # Environment variable template
â”œâ”€â”€ .gitignore                       # Git exclusion rules
â”‚
â”œâ”€â”€ .chainlit/
â”‚   â””â”€â”€ config.toml                  # UI name, theme, port
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_vectorstore.py         # PDF â†’ chunks â†’ embeddings â†’ FAISS index
â”‚   â”œâ”€â”€ test_rag_quick.py            # Single end-to-end RAG validation
â”‚   â””â”€â”€ test_full_rag.py             # 5-stage comprehensive pipeline test
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py                # Single source of truth for all settings
â”‚   â”‚   â””â”€â”€ logger.py                # File + console logging with timers
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py         # PDF loading, extraction, metadata
â”‚   â”‚   â””â”€â”€ text_splitter.py         # Recursive chunking (size=1000, overlap=200)
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ embeddings.py            # Sentence-Transformer wrapper
â”‚   â”‚   â””â”€â”€ vector_search.py         # FAISS: create, save, load, search
â”‚   â””â”€â”€ generation/
â”‚       â”œâ”€â”€ llm_handler.py           # Flan-T5 loading + text generation
â”‚       â””â”€â”€ prompt_templates.py      # QA prompts + source formatting
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Place encyclopedia PDF here
â”‚   â””â”€â”€ processed/                   # Reserved for preprocessing output
â”‚
â”œâ”€â”€ vectorstore/                     # FAISS index â€” auto-generated, git-ignored
â”œâ”€â”€ logs/                            # Runtime logs â€” git-ignored
â””â”€â”€ assets/
    â””â”€â”€ screenshots/                 # README visuals
```

---

## â–¶ï¸ Setup & Usage

### 1. Create Environment

```bash
conda create -n medibot python=3.11 -y
conda activate medibot
pip install -r requirements.txt
```

### 2. Add the Medical PDF

Place the Gale Encyclopedia PDF into the raw data folder:

```bash
cp /path/to/gale_encyclopedia_vol1.pdf data/raw/
```

### 3. Build the Vector Store

This is the core preprocessing step. It reads the PDF, chunks the text, embeds every chunk, and saves a searchable FAISS index.

```bash
python scripts/build_vectorstore.py
```

Expected output:

```
Step 1/4: Processing PDF         â†’  ~700 pages loaded
Step 2/4: Splitting into chunks  â†’  ~3,400 chunks created
Step 3/4: Creating vector store  â†’  Embeddings generated
Step 4/4: Saving to disk         â†’  vectorstore/medibot_faiss_index/

âœ… Vector store built successfully
```

### 4. Validate the Pipeline

Before launching the UI, run the end-to-end test to confirm retrieval and generation work together:

```bash
python scripts/test_rag_quick.py
```

If the full pipeline is working, you will see:

```
âœ… RAG TEST SUCCESSFUL
```

### 5. Run the Application

```bash
chainlit run main.py
```

Open in your browser:

```
http://localhost:8000
```

---

## ğŸ’¬ Example Questions

These topics fall within the Aâ€“B coverage of Volume 1:

| Question | Why It Works |
|---|---|
| *What are the symptoms of asthma?* | Asthma â†’ Aâ€“B scope âœ“ |
| *How is bronchitis diagnosed?* | Bronchitis â†’ Aâ€“B scope âœ“ |
| *What causes appendicitis?* | Appendicitis â†’ Aâ€“B scope âœ“ |
| *Explain Addison's disease* | Addison's â†’ Aâ€“B scope âœ“ |

Every response includes a grounded answer, cited source pages, and an educational disclaimer.

---

## ğŸ§ª Testing

### Quick Validation

```bash
python scripts/test_rag_quick.py
```

Runs one full query end-to-end. Confirms: **load â†’ search â†’ generate â†’ respond**.

### Full Pipeline Test

```bash
python scripts/test_full_rag.py
```

| Stage | What It Verifies |
|---|---|
| 1 | Vector search returns relevant chunks |
| 2 | LLM generates coherent text independently |
| 3 | Manual RAG pipeline works step by step |
| 4 | LangChain RetrievalQA chain works end-to-end |
| 5 | Edge cases and error handling are robust |

### Safety Checks

| Check | Purpose |
|---|---|
| Keyword grounding verification | Confirms answer relates to retrieved text |
| Context-overlap enforcement | Detects when model ignores context |
| Missing-data refusal | Returns safe fallback instead of guessing |
| Source tracking | Page numbers preserved from PDF through to response |

---

## âš ï¸ Limitations

| Limitation | Details |
|---|---|
| **Corpus scope** | Aâ€“B topics only (Volume 1) |
| **No live data** | Static encyclopedia â€” no internet access |
| **Not diagnostic** | Educational use only |
| **Model size** | Flan-T5-base is lightweight; larger models produce better answers |
| **Language** | English only |
| **First query latency** | 10â€“30s on first run (model loading); 2â€“5s after that |

---

## âš•ï¸ Medical Disclaimer

> **Medibot is for educational and research purposes only.**
>
> It does not provide medical advice, diagnosis, or treatment recommendations. It is not a medical professional and cannot replace doctors, clinicians, or healthcare providers.
>
> **Always consult a qualified healthcare professional for any medical decisions.**

---

## ğŸ¯ Why This Project Matters

Medibot demonstrates three principles that define responsible AI in sensitive domains:

**1. Retrieval reduces hallucination.**
By forcing the model to read relevant text before answering, fabricated responses are dramatically reduced. The model doesn't guess â€” it reads, then answers.

**2. Knowing when not to answer is critical.**
A system that confidently gives wrong medical information is worse than one that says "I don't know." Medibot is designed around this principle.

**3. Source transparency builds trust.**
Every answer shows exactly where it came from â€” document name, page number. Users can verify. That's how trust works.

These are the same principles behind production medical AI systems. Medibot applies them at a scale that's clear, reproducible, and learnable.
---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## ğŸ“¬ Contact

| Platform | Link |
|---|---|
| GitHub | [Your GitHub](https://github.com/yourusername) |
| LinkedIn | [Your LinkedIn](https://linkedin.com/in/yourprofile) |
| Email | your.email@example.com |

---

<p align="center">
<em>Retrieves first. Generates second. Cites always.</em>
</p>